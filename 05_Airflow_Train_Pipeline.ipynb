{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6459bfbe",
   "metadata": {},
   "source": [
    "## Apache Airflow with Kubeflow\n",
    "\n",
    "In this section of the hands-on-lab, we will utilize Snowpark's Python client-side Dataframe API as well as Kubeflow  and Apache Airflow to create an operational pipeline.  We will take the functions created by the ML Ops team and create a directed acyclic graph (DAG) of operations to run each month when new data is available. \n",
    "\n",
    "Note: This code requires the ability to run docker containers and kubernetes locally."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c015b2",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Kubeflow works with Kubernetes version 1.21 and before.  This requires installing an older version of Docker Desktop  \n",
    "Install Docker Desktop 4.1.0 (with Kubernetes 1.21.5)\n",
    "```bash\n",
    "curl -LO https://desktop.docker.com/mac/main/amd64/69386/Docker.dmg\n",
    "```\n",
    "\n",
    "We need to downgrade the kubernetes version due to a bug with swagger vs openapi.\n",
    "```bash\n",
    "pip install -Iq kubernetes==10.0.1\n",
    "```\n",
    "\n",
    "Install kubectl in local directory to ensure we use the correct version for kubernetes 1.21.5 in Docker Desktop\n",
    "```bash\n",
    "curl -LO \"https://dl.k8s.io/release/v1.21.5/bin/darwin/amd64/kubectl\"; chmod u+x kubectl\n",
    "./kubectl version\n",
    "```\n",
    "\n",
    "Install Kubeflow training operators.  Rather than the entire kubeflow distribution we will only use PytorchJob.\n",
    "```bash\n",
    "./kubectl apply -k \"github.com/kubeflow/training-operator/manifests/overlays/standalone?ref=v1.3.0\"\n",
    "```\n",
    "\n",
    "Optionally install kubernetes dashboard\n",
    "```bash\n",
    "./kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.6.0/aio/deploy/recommended.yaml\n",
    "```\n",
    "\n",
    "##IMPORTANT:  \n",
    "Copy kube_config for Airflow to use in DAG\n",
    "```bash\n",
    "cp ~/.kube/config ./include/.kube/config\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176da817",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup docker token \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "832980e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7e3250",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee385179",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8114e4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a73ab74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174a5fd0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1813ed8a",
   "metadata": {},
   "source": [
    "Create a state dictionary to use during testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ac56c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "\n",
    "model_id = '03A08400_EE3C_11EC_A5EE_ACDE48001122'\n",
    "\n",
    "password = getpass.getpass('Enter password: ')\n",
    "account = getpass.getpass('Enter account: ')\n",
    "state_dict = {\"connection_parameters\": {\"password\": password},\n",
    "              \"compute_parameters\" : {\"default_warehouse\": \"XSMALL_WH\"}\n",
    "             }\n",
    "state_dict['connection_parameters']['user'] = 'jack' \n",
    "state_dict['connection_parameters']['account'] = account\n",
    "state_dict['connection_parameters']['role']='PUBLIC'\n",
    "state_dict['connection_parameters']['database']='CITIBIKEML_jack'\n",
    "state_dict['connection_parameters']['schema']='DEMO'\n",
    "state_dict['feature_table_name']='FEATURE_'+model_id\n",
    "state_dict['pred_table_name']='PRED_'+model_id\n",
    "state_dict['model_file_name']='forecast_model.zip'\n",
    "state_dict['le_file_name']='label_encoders.pkl'\n",
    "state_dict['cat_cols'] = ['STATION_ID', 'HOLIDAY']\n",
    "state_dict['k8s_namespace'] = 'citibike'\n",
    "state_dict['train_image'] = 'docker.io/mpgregor/airkube:latest'\n",
    "state_dict['train_job_name'] = 'citibike-train-'+model_id.replace('_', '-').lower()\n",
    "\n",
    "import json\n",
    "with open('./include/state.json', 'w') as sdf:\n",
    "    json.dump(state_dict, sdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d81cae0e",
   "metadata": {},
   "source": [
    "The following python script will be built-in to a Docker container.  See `training/Dockerfile` for details.  The Github Actions script is located at `.github/workflows/docker-image.yml`.  Commiting any changes to the following python code will trigger a Docker image rebuild."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33cd0950",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile training/load_train.py\n",
    "import argparse\n",
    "\n",
    "def load_and_encode(state_dict):\n",
    "\n",
    "    from snowflake import snowpark as snp\n",
    "    import pandas as pd\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    from collections import defaultdict\n",
    "    import pickle\n",
    "    \n",
    "    session = snp.Session.builder.configs(state_dict['connection_parameters']).create()\n",
    "    session.use_warehouse(state_dict['compute_parameters']['default_warehouse'])\n",
    "\n",
    "    feature_df = session.table(state_dict['feature_table_name']).to_pandas()\n",
    "    #forecast_df = session.table(state_dict['forecast_table_name']).to_pandas()\n",
    "\n",
    "    session.close()\n",
    "\n",
    "    feature_df['DATE'] = pd.to_datetime(feature_df['DATE'])\n",
    "    feature_df.set_index('DATE', inplace=True)\n",
    "    \n",
    "    #forecast_df['DATE'] = pd.to_datetime(forecast_df['DATE'])\n",
    "    #forecast_df.set_index('DATE', inplace=True)\n",
    "\n",
    "    cat_cols = state_dict['cat_cols']\n",
    "    num_cols = [set(feature_df.columns)-set(cat_cols)]\n",
    "    state_dict['num_cols'] = num_cols\n",
    "\n",
    "    try:\n",
    "        with open(state_dict['le_file_name'], 'rb') as fh: \n",
    "            d=pickle.load(fh)\n",
    "        feature_df[cat_cols]=feature_df[cat_cols].apply(lambda x: d[x.name].transform(x))\n",
    "\n",
    "    except: \n",
    "        d = defaultdict(LabelEncoder)\n",
    "        feature_df[cat_cols]=feature_df[cat_cols].apply(lambda x: d[x.name].fit_transform(x))\n",
    "\n",
    "        with open(state_dict['le_file_name'], 'wb') as fh: \n",
    "            pickle.dump(d, fh)\n",
    "\n",
    "    return state_dict, feature_df\n",
    "\n",
    "def train_and_save(state_dict, feature_df):\n",
    "    import pandas as pd\n",
    "    from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "    \n",
    "    feature_df.sort_values(by='DATE', ascending=True, inplace=True)\n",
    "\n",
    "    train_df = feature_df.groupby('STATION_ID').head(-365)\n",
    "    valid_df = feature_df.groupby('STATION_ID').tail(365)\n",
    "\n",
    "    state_dict['cat_idxs'] = [feature_df.drop(columns=['COUNT'], axis=1).columns.get_loc(col) for col in state_dict['cat_cols']]\n",
    "    state_dict['cat_dims'] = list(feature_df.drop(columns=['COUNT'], axis=1).iloc[:, state_dict['cat_idxs']].nunique().values)\n",
    "\n",
    "    y_train = train_df['COUNT'].values.reshape(-1,1)\n",
    "    X_train = train_df.drop(columns ='COUNT', axis=1).values\n",
    "\n",
    "    y_valid = valid_df['COUNT'].values.reshape(-1,1)\n",
    "    X_valid = valid_df.drop(columns ='COUNT', axis=1).values\n",
    "    \n",
    "    model = TabNetRegressor(cat_idxs=state_dict['cat_idxs'], cat_dims=state_dict['cat_dims'])\n",
    "\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_valid, y_valid)],\n",
    "        max_epochs=1,\n",
    "        patience=100,\n",
    "        batch_size=2048, \n",
    "        virtual_batch_size=256,\n",
    "        num_workers=0,\n",
    "        drop_last=True)\n",
    "\n",
    "    model.save_model(state_dict['model_file_name'].split('.')[0])\n",
    "    \n",
    "    return state_dict\n",
    "\n",
    "def pred(state_dict, feature_df):\n",
    "    from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "    import pandas as pd\n",
    "    from torch import tensor\n",
    "    \n",
    "    model = TabNetRegressor(cat_idxs=state_dict['cat_idxs'], cat_dims=state_dict['cat_dims'])\n",
    "\n",
    "    model.load_model(state_dict['model_file_name'])\n",
    "    \n",
    "    pred_df = feature_df.copy(deep=True)\n",
    "    \n",
    "    pred_df['PRED'] = model.predict(tensor(feature_df.drop(columns=['COUNT']).values)).round().astype('int')\n",
    "    \n",
    "    return state_dict, pred_df\n",
    "\n",
    "def forecast(state_dict, feature_df, forecast_df):\n",
    "\n",
    "    if len(state_dict['lag_values']) > 0:\n",
    "        for step in range(state_dict['forecast_steps']):\n",
    "            #station_id = df.iloc[-1]['STATION_ID']\n",
    "            future_date = df.iloc[-1]['DATE']+timedelta(days=1)\n",
    "            lags=[df.shift(lag-1).iloc[-1]['COUNT'] for lag in state_dict['lag_values']]\n",
    "            forecast=forecast_df.loc[forecast_df['DATE']==future_date.strftime('%Y-%m-%d')]\n",
    "            forecast=forecast.drop(labels='DATE', axis=1).values.tolist()[0]\n",
    "            features=[*lags, *forecast]\n",
    "            pred=round(model.predict(np.array([features]))[0][0])\n",
    "            row=[future_date, pred, *features, pred]\n",
    "            df.loc[len(df)]=row\n",
    "\n",
    "    return state_dict, pred_df\n",
    "\n",
    "def decode_and_write(state_dict, pred_df):\n",
    "    from snowflake import snowpark as snp\n",
    "    import pandas as pd\n",
    "    import pickle\n",
    "    \n",
    "    with open(state_dict['le_file_name'], 'rb') as fh: \n",
    "        d=pickle.load(fh)\n",
    "\n",
    "    pred_df[state_dict['cat_cols']] = pred_df[state_dict['cat_cols']].apply(lambda x: d[x.name].inverse_transform(x))\n",
    "\n",
    "\n",
    "    session = snp.Session.builder.configs(state_dict['connection_parameters']).create()\n",
    "    session.use_warehouse(state_dict['compute_parameters']['default_warehouse'])\n",
    "\n",
    "    session.create_dataframe(pred_df).write.mode('overwrite').save_as_table(state_dict['pred_table_name'])\n",
    "    \n",
    "    session.close()\n",
    "    \n",
    "    return state_dict\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # Defining and parsing the command-line arguments\n",
    "    parser = argparse.ArgumentParser(description='airkube training')\n",
    "    parser.add_argument('--password', type=str)\n",
    "    parser.add_argument('--account', type=str)\n",
    "    parser.add_argument('--username', type=str)\n",
    "    parser.add_argument('--role', type=str)\n",
    "    parser.add_argument('--database', type=str)\n",
    "    parser.add_argument('--schema', type=str)\n",
    "    parser.add_argument('--feature_table_name', type=str)\n",
    "    parser.add_argument('--pred_table_name', type=str)\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Creating the directory where the output file will be created (the directory may or may not exist).\n",
    "    #Path(args.accuracy).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    state_dict = {\"connection_parameters\": {\"password\": args.password},\n",
    "                  \"compute_parameters\" : {\"default_warehouse\": \"XSMALL_WH\"}}\n",
    "    state_dict['connection_parameters']['user'] = args.username\n",
    "    state_dict['connection_parameters']['account'] = args.account\n",
    "    state_dict['connection_parameters']['role'] = args.role\n",
    "    state_dict['connection_parameters']['database'] = args.database\n",
    "    state_dict['connection_parameters']['schema'] = args.schema\n",
    "    state_dict['feature_table_name'] = args.feature_table_name\n",
    "    state_dict['pred_table_name'] = args.pred_table_name\n",
    "    state_dict['model_file_name']='forecast_model.zip'\n",
    "    state_dict['le_file_name']='label_encoders.pkl'\n",
    "    state_dict[\"cat_cols\"] = ['STATION_ID', 'HOLIDAY']\n",
    "\n",
    "    load_state_dict, feature_df = load_and_encode(state_dict)\n",
    "    train_state_dict = train_and_save(load_state_dict, feature_df)\n",
    "    pred_state_dict, pred_df = pred(state_dict, feature_df)\n",
    "    state_dict = decode_and_write(state_dict, pred_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d68c629",
   "metadata": {},
   "source": [
    "### Test the python code in local kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf33f62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_train import load_and_encode, train_and_save, pred, decode_and_write\n",
    "import json\n",
    "\n",
    "with open('./include/state.json') as sdf:\n",
    "    state_dict = json.load(sdf)    \n",
    "\n",
    "load_state_dict, feature_df = load_and_encode(state_dict)\n",
    "train_state_dict = train_and_save(load_state_dict, feature_df)\n",
    "pred_state_dict, pred_df = pred(state_dict, feature_df)\n",
    "state_dict = decode_and_write(state_dict, pred_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd8e38a",
   "metadata": {},
   "source": [
    "Check output table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00450a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from snowflake import snowpark as snp\n",
    "\n",
    "session = snp.Session.builder.configs(state_dict['connection_parameters']).create()\n",
    "session.use_warehouse(state_dict['compute_parameters']['default_warehouse'])\n",
    "session.table(state_dict['pred_table_name']).show()\n",
    "session.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "085f040e",
   "metadata": {},
   "source": [
    "### Test with kubectl CLI and YAML file specification for `PytorchJob`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089c96f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile ./include/train.yaml\n",
    "apiVersion: \"kubeflow.org/v1\"\n",
    "kind: PyTorchJob\n",
    "metadata:\n",
    "  name: citibike-train\n",
    "  namespace: citibike\n",
    "spec:\n",
    "  pytorchReplicaSpecs:\n",
    "    Master:\n",
    "      replicas: 1\n",
    "      restartPolicy: Never\n",
    "      template:\n",
    "        spec:\n",
    "          containers:\n",
    "            - name: pytorch\n",
    "              image: docker.io/mpgregor/airkube:latest\n",
    "              imagePullPolicy: Always\n",
    "              command:\n",
    "                - \"python\"\n",
    "                - \"/pipeline/load_train.py\"\n",
    "                - \"--account=\"\n",
    "                - \"--password=\"\n",
    "                - \"--username=\"\n",
    "                - \"--role=\"\n",
    "                - \"--database=\"\n",
    "                - \"--schema=\"\n",
    "                - \"--feature_table_name=\"\n",
    "                - \"--pred_table_name=\"            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5488d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !kubectl create namespace citibike\n",
    "# !kubectl create -f ./include/train.yaml \n",
    "# !kubectl delete pytorchjob citibike-train -n citibike\n",
    "# !kubectl delete namespace citibike    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c57aad3",
   "metadata": {},
   "source": [
    "### Test with Apache Airflow and Kubernetes `KubernetesPodOperator`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b74564",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./dags/k8s_test.py\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from airflow import DAG\n",
    "from airflow.configuration import conf\n",
    "from airflow.providers.cncf.kubernetes.operators.kubernetes_pod import KubernetesPodOperator\n",
    "\n",
    "default_args = {\n",
    "    'owner': 'airflow',\n",
    "    'depends_on_past': False,\n",
    "    'start_date': datetime(2019, 1, 1),\n",
    "    'email_on_failure': False,\n",
    "    'email_on_retry': False,\n",
    "    'retries': 1,\n",
    "    'retry_delay': timedelta(minutes=5),\n",
    "}\n",
    "\n",
    "namespace = conf.get('kubernetes', 'NAMESPACE')\n",
    "\n",
    "# This will detect the default namespace locally and read the\n",
    "# environment namespace when deployed to Astronomer.\n",
    "if namespace =='default':\n",
    "    config_file = '/usr/local/airflow/include/.kube/config'\n",
    "    in_cluster = False\n",
    "else:\n",
    "    in_cluster = True\n",
    "    config_file = None\n",
    "\n",
    "dag = DAG('citibike_k8s_train', schedule_interval='@once', default_args=default_args)\n",
    "\n",
    "import json\n",
    "with open('/usr/local/airflow/include/state.json') as sdf:\n",
    "    state_dict = json.load(sdf)\n",
    "\n",
    "with dag:\n",
    "    KubernetesPodOperator(\n",
    "        namespace=namespace,\n",
    "        image=state_dict['train_image'],\n",
    "        cmds=[\"python\", \n",
    "             \"/pipeline/load_train.py\",\n",
    "             \"--account=\"+state_dict['connection_parameters']['account'], \n",
    "             \"--password=\"+state_dict['connection_parameters']['password'],\n",
    "             \"--username=\"+state_dict['connection_parameters']['user'],\n",
    "             \"--role=\"+state_dict['connection_parameters']['role'], \n",
    "             \"--database=\"+state_dict['connection_parameters']['database'], \n",
    "             \"--schema=\"+state_dict['connection_parameters']['schema'], \n",
    "             \"--feature_table_name=\"+state_dict['feature_table_name'], \n",
    "             \"--pred_table_name=\"+state_dict['pred_table_name']\n",
    "            ],\n",
    "        labels={\"foo\": \"bar\"},\n",
    "        name=\"citibike-train\",\n",
    "        task_id=\"train\",\n",
    "        in_cluster=in_cluster,  # if set to true, will look in the cluster, if false, looks for file\n",
    "        cluster_context=\"docker-desktop\",  # is ignored when in_cluster is set to True\n",
    "        config_file=config_file,\n",
    "        is_delete_operator_pod=True,\n",
    "        get_logs=True,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb1f5ee",
   "metadata": {},
   "source": [
    "### Test with Kubeflow `Pytorchjob` from local kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a752458d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kubernetes.client import V1PodTemplateSpec\n",
    "from kubernetes.client import V1ObjectMeta\n",
    "from kubernetes.client import V1PodSpec\n",
    "from kubernetes.client import V1Container\n",
    "from kubernetes.client import V1ResourceRequirements\n",
    "\n",
    "from kubeflow.pytorchjob import constants\n",
    "from kubeflow.pytorchjob import utils\n",
    "from kubeflow.pytorchjob import V1ReplicaSpec\n",
    "from kubeflow.pytorchjob import V1PyTorchJob\n",
    "from kubeflow.pytorchjob import V1PyTorchJobSpec\n",
    "from kubeflow.pytorchjob import PyTorchJobClient\n",
    "\n",
    "#namespace = state_dict['k8s_namespace'] #utils.get_default_target_namespace()\n",
    "\n",
    "import json\n",
    "with open('./include/state.json') as sdf:\n",
    "    state_dict = json.load(sdf)\n",
    "\n",
    "from kubernetes import client, config\n",
    "\n",
    "config_file = './include/.kube/config'\n",
    "\n",
    "config.load_kube_config(config_file)\n",
    "\n",
    "k8s_client = client.CoreV1Api()\n",
    "if state_dict['k8s_namespace'] not in [item.metadata.name for item in k8s_client.list_namespace().items]:\n",
    "    k8s_client.create_namespace(client.V1Namespace(metadata=client.V1ObjectMeta(name=state_dict['k8s_namespace'])))\n",
    "\n",
    "container = V1Container(\n",
    "    name=\"pytorch\",\n",
    "    image=state_dict['train_image'],\n",
    "    image_pull_policy=\"Always\",\n",
    "    command=[\"python\", \n",
    "             \"/pipeline/load_train.py\",\n",
    "             \"--account=\"+state_dict['connection_parameters']['account'], \n",
    "             \"--password=\"+state_dict['connection_parameters']['password'],\n",
    "             \"--username=\"+state_dict['connection_parameters']['user'],\n",
    "             \"--role=\"+state_dict['connection_parameters']['role'], \n",
    "             \"--database=\"+state_dict['connection_parameters']['database'], \n",
    "             \"--schema=\"+state_dict['connection_parameters']['schema'], \n",
    "             \"--feature_table_name=\"+state_dict['feature_table_name'], \n",
    "             \"--pred_table_name=\"+state_dict['pred_table_name']\n",
    "            ]\n",
    ")\n",
    "\n",
    "master = V1ReplicaSpec(\n",
    "    replicas=1,\n",
    "    restart_policy=\"OnFailure\",\n",
    "    template=V1PodTemplateSpec(\n",
    "        spec=V1PodSpec(\n",
    "            containers=[container]\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "worker = V1ReplicaSpec(\n",
    "    replicas=1,\n",
    "    restart_policy=\"OnFailure\",\n",
    "    template=V1PodTemplateSpec(\n",
    "        spec=V1PodSpec(\n",
    "            containers=[container]\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "pytorchjob = V1PyTorchJob(\n",
    "    api_version=\"kubeflow.org/v1\",\n",
    "    kind=\"PyTorchJob\",\n",
    "    metadata=V1ObjectMeta(name=state_dict['train_job_name'], namespace=state_dict['k8s_namespace']),\n",
    "    spec=V1PyTorchJobSpec(\n",
    "        clean_pod_policy=\"None\",\n",
    "        pytorch_replica_specs={\"Master\": master} \n",
    "    )\n",
    ")\n",
    "\n",
    "pytorch_client = PyTorchJobClient()\n",
    "resp = pytorch_client.create(pytorchjob)\n",
    "\n",
    "resp = pytorch_client.wait_for_condition(name=resp['metadata']['name'], \n",
    "                                       namespace=resp['metadata']['namespace'],\n",
    "                                       expected_condition='Succeeded')\n",
    "\n",
    "pytorch_client.delete(name=resp['metadata']['name'], \n",
    "                      namespace=resp['metadata']['namespace'])\n",
    "\n",
    "state_dict['pytorchjob_uid'] = resp['metadata']['uid']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02997be",
   "metadata": {},
   "source": [
    "### Test with Airflow `Taskflow` Operator for `PytorchJob`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd232d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./dags/k8s_test.py\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from airflow.decorators import dag, task\n",
    "\n",
    "default_args = {\n",
    "    'owner': 'airflow',\n",
    "    'depends_on_past': False,\n",
    "    'email_on_failure': False,\n",
    "    'email_on_retry': False,\n",
    "    'retries': 1,\n",
    "    'retry_delay': timedelta(minutes=5)\n",
    "}\n",
    "\n",
    "#local_airflow_path = '/usr/local/airflow/'\n",
    "\n",
    "@task.virtualenv(python_version=3.8)\n",
    "def batch_train_k8s(state_dict:dict)-> dict:\n",
    "\n",
    "    from kubernetes.client import V1PodTemplateSpec\n",
    "    from kubernetes.client import V1ObjectMeta\n",
    "    from kubernetes.client import V1PodSpec\n",
    "    from kubernetes.client import V1Container\n",
    "    from kubernetes.client import V1ResourceRequirements\n",
    "    from kubernetes.client import CoreV1Api\n",
    "    from kubeflow.pytorchjob import V1ReplicaSpec\n",
    "    from kubeflow.pytorchjob import V1PyTorchJob\n",
    "    from kubeflow.pytorchjob import V1PyTorchJobSpec\n",
    "    from kubeflow.pytorchjob import PyTorchJobClient\n",
    "    from kubernetes import config\n",
    "\n",
    "    config_file = './include/.kube/config'\n",
    "    \n",
    "    config.load_kube_config(config_file)\n",
    "\n",
    "    k8s_client = CoreV1Api()\n",
    "    if state_dict['k8s_namespace'] not in [item.metadata.name for item in k8s_client.list_namespace().items]:\n",
    "        k8s_client.create_namespace(client.V1Namespace(metadata=client.V1ObjectMeta(name=state_dict['k8s_namespace'])))\n",
    "\n",
    "    container = V1Container(\n",
    "        name=\"pytorch\",\n",
    "        image=state_dict['train_image'],\n",
    "        image_pull_policy=\"Always\",\n",
    "        command=[\"python\", \n",
    "                 \"/pipeline/load_train.py\",\n",
    "                 \"--account=\"+state_dict['connection_parameters']['account'], \n",
    "                 \"--password=\"+state_dict['connection_parameters']['password'],\n",
    "                 \"--username=\"+state_dict['connection_parameters']['user'],\n",
    "                 \"--role=\"+state_dict['connection_parameters']['role'], \n",
    "                 \"--database=\"+state_dict['connection_parameters']['database'], \n",
    "                 \"--schema=\"+state_dict['connection_parameters']['schema'], \n",
    "                 \"--feature_table_name=\"+state_dict['feature_table_name'], \n",
    "                 \"--pred_table_name=\"+state_dict['pred_table_name']\n",
    "                ]\n",
    "    )\n",
    "\n",
    "    master = V1ReplicaSpec(\n",
    "        replicas=1,\n",
    "        restart_policy=\"OnFailure\",\n",
    "        template=V1PodTemplateSpec(\n",
    "            spec=V1PodSpec(\n",
    "                containers=[container]\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "    worker = V1ReplicaSpec(\n",
    "        replicas=1,\n",
    "        restart_policy=\"OnFailure\",\n",
    "        template=V1PodTemplateSpec(\n",
    "            spec=V1PodSpec(\n",
    "                containers=[container]\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "    pytorchjob = V1PyTorchJob(\n",
    "        api_version=\"kubeflow.org/v1\",\n",
    "        kind=\"PyTorchJob\",\n",
    "        metadata=V1ObjectMeta(name=state_dict['train_job_name'], namespace=state_dict['k8s_namespace']),\n",
    "        spec=V1PyTorchJobSpec(\n",
    "            clean_pod_policy=\"None\",\n",
    "            pytorch_replica_specs={\"Master\": master} \n",
    "        )\n",
    "    )\n",
    "\n",
    "    pytorch_client = PyTorchJobClient(config_file=config_file)\n",
    "    resp = pytorch_client.create(pytorchjob)\n",
    "\n",
    "    resp = pytorch_client.wait_for_condition(name=resp['metadata']['name'], \n",
    "                                           namespace=resp['metadata']['namespace'],\n",
    "                                           expected_condition='Succeeded')\n",
    "\n",
    "    state_dict['pytorchjob_uid'] = resp['metadata']['uid']\n",
    "\n",
    "    return state_dict\n",
    "\n",
    "@dag(default_args=default_args, schedule_interval=None, start_date=datetime(2020, 3, 1), catchup=False, tags=['setup'])\n",
    "def citibikeml_test_taskflow(run_date:str):\n",
    "    \"\"\"\n",
    "    Test pytorchjob for batch training\n",
    "    \"\"\"\n",
    "    import uuid\n",
    "    import json\n",
    "    \n",
    "    with open('./include/state.json') as sdf:\n",
    "        state_dict = json.load(sdf)\n",
    "    \n",
    "    model_id = '03A08400_EE3C_11EC_A5EE_ACDE48001122'\n",
    "\n",
    "    state_dict.update({'model_id': model_id})\n",
    "    state_dict.update({'run_date': run_date})\n",
    "    state_dict.update({'feature_table_name' : 'FEATURE_'+model_id,\n",
    "                       'pred_table_name': 'PRED_'+model_id,\n",
    "                       'forecast_table_name': 'FORECAST_'+model_id,\n",
    "                       'forecast_steps': 30,\n",
    "                       'model_file_name' : 'forecast_model.zip',\n",
    "                       'le_file_name' : 'label_encoders.pkl',\n",
    "                       'cat_cols' : ['STATION_ID', 'HOLIDAY'],\n",
    "                       'k8s_namespace' : 'citibike',\n",
    "                       'train_image' : 'docker.io/mpgregor/airkube:latest',\n",
    "                       'train_job_name' : 'citibike-train-'+model_id.replace('_', '-').lower()\n",
    "                      })\n",
    "\n",
    "    return batch_train_k8s(state_dict)\n",
    "\n",
    "run_date='2020_01_01'\n",
    "\n",
    "state_dict = citibikeml_test_taskflow(run_date=run_date)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab4c7c13",
   "metadata": {},
   "source": [
    "Save task to dags directory for Airflow integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89da52eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile dags/airflow_kubeflow.py\n",
    "\n",
    "def batch_train_pred_k8s(state_dict:dict)-> dict:\n",
    "\n",
    "    from kubernetes.client import V1PodTemplateSpec\n",
    "    from kubernetes.client import V1ObjectMeta\n",
    "    from kubernetes.client import V1PodSpec\n",
    "    from kubernetes.client import V1Container\n",
    "    from kubernetes.client import V1ResourceRequirements\n",
    "    from kubernetes.client import CoreV1Api\n",
    "    from kubeflow.pytorchjob import V1ReplicaSpec\n",
    "    from kubeflow.pytorchjob import V1PyTorchJob\n",
    "    from kubeflow.pytorchjob import V1PyTorchJobSpec\n",
    "    from kubeflow.pytorchjob import PyTorchJobClient\n",
    "    from kubernetes import config\n",
    "\n",
    "    config_file = './include/.kube/config'\n",
    "    \n",
    "    config.load_kube_config(config_file)\n",
    "\n",
    "    k8s_client = CoreV1Api()\n",
    "    if state_dict['k8s_namespace'] not in [item.metadata.name for item in k8s_client.list_namespace().items]:\n",
    "        k8s_client.create_namespace(client.V1Namespace(metadata=client.V1ObjectMeta(name=state_dict['k8s_namespace'])))\n",
    "\n",
    "    container = V1Container(\n",
    "        name=\"pytorch\",\n",
    "        image=state_dict['train_image'],\n",
    "        image_pull_policy=\"Always\",\n",
    "        command=[\"python\", \n",
    "                 \"/pipeline/load_train.py\",\n",
    "                 \"--account=\"+state_dict['connection_parameters']['account'], \n",
    "                 \"--password=\"+state_dict['connection_parameters']['password'],\n",
    "                 \"--username=\"+state_dict['connection_parameters']['user'],\n",
    "                 \"--role=\"+state_dict['connection_parameters']['role'], \n",
    "                 \"--database=\"+state_dict['connection_parameters']['database'], \n",
    "                 \"--schema=\"+state_dict['connection_parameters']['schema'], \n",
    "                 \"--feature_table_name=\"+state_dict['feature_table_name'], \n",
    "                 \"--pred_table_name=\"+state_dict['pred_table_name']\n",
    "                ]\n",
    "    )\n",
    "\n",
    "    master = V1ReplicaSpec(\n",
    "        replicas=1,\n",
    "        restart_policy=\"OnFailure\",\n",
    "        template=V1PodTemplateSpec(\n",
    "            spec=V1PodSpec(\n",
    "                containers=[container]\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "    worker = V1ReplicaSpec(\n",
    "        replicas=1,\n",
    "        restart_policy=\"OnFailure\",\n",
    "        template=V1PodTemplateSpec(\n",
    "            spec=V1PodSpec(\n",
    "                containers=[container]\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "    pytorchjob = V1PyTorchJob(\n",
    "        api_version=\"kubeflow.org/v1\",\n",
    "        kind=\"PyTorchJob\",\n",
    "        metadata=V1ObjectMeta(name=state_dict['train_job_name'], namespace=state_dict['k8s_namespace']),\n",
    "        spec=V1PyTorchJobSpec(\n",
    "            clean_pod_policy=\"None\",\n",
    "            pytorch_replica_specs={\"Master\": master} \n",
    "        )\n",
    "    )\n",
    "\n",
    "    pytorch_client = PyTorchJobClient(config_file=config_file)\n",
    "    resp = pytorch_client.create(pytorchjob)\n",
    "\n",
    "    resp = pytorch_client.wait_for_condition(name=resp['metadata']['name'], \n",
    "                                           namespace=resp['metadata']['namespace'],\n",
    "                                           expected_condition='Succeeded')\n",
    "\n",
    "    state_dict['pytorchjob_uid'] = resp['metadata']['uid']\n",
    "\n",
    "    return state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd86a0bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
