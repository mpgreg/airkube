{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e221e34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969ecec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "##setup\n",
    "#!pip install -Iq kubernetes==10.0.1\n",
    "#!curl -LO \"https://dl.k8s.io/release/v1.21.5/bin/darwin/amd64/kubectl\"; chmod u+x kubectl\n",
    "#!./kubectl version\n",
    "#!./kubectl apply -k \"github.com/kubeflow/training-operator/manifests/overlays/standalone?ref=v1.3.0\"\n",
    "#!./kubectl apply -f https://raw.githubusercontent.com/kubernetes/dashboard/v2.6.0/aio/deploy/recommended.yaml\n",
    "!cp ~/.kube/config ./include/.kube/config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20bb411",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ac56c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "password = getpass.getpass('Enter password: ')\n",
    "account = getpass.getpass('Enter account: ')\n",
    "state_dict = {\"connection_parameters\": {\"password\": password},\n",
    "              \"compute_parameters\" : {\"default_warehouse\": \"XSMALL_WH\"}\n",
    "             }\n",
    "state_dict['connection_parameters']['user'] = 'jack' \n",
    "state_dict['connection_parameters']['account'] = account\n",
    "state_dict['connection_parameters']['role']='PUBLIC'\n",
    "state_dict['connection_parameters']['database']='CITIBIKEML_jack'\n",
    "state_dict['connection_parameters']['schema']='DEMO'\n",
    "state_dict['feature_table_name']='FEATURE_03A08400_EE3C_11EC_A5EE_ACDE48001122'\n",
    "state_dict['pred_table_name']='PRED_03A08400_EE3C_11EC_A5EE_ACDE48001122'\n",
    "state_dict['model_file_name']='forecast_model.zip'\n",
    "state_dict['le_file_name']='label_encoders.pkl'\n",
    "state_dict['cat_cols'] = ['STATION_ID', 'HOLIDAY']\n",
    "state_dict['k8s_namespace'] = 'citibike'\n",
    "state_dict['train_image'] = 'docker.io/mpgregor/airkube:latest'\n",
    "state_dict['train_job_name'] = 'citibike-train'\n",
    "\n",
    "import json\n",
    "with open('./include/state.json', 'w') as sdf:\n",
    "    json.dump(state_dict, sdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33cd0950",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile training/load_train.py\n",
    "import argparse\n",
    "\n",
    "def load_and_encode(state_dict):\n",
    "\n",
    "    from snowflake import snowpark as snp\n",
    "    import pandas as pd\n",
    "    from sklearn.preprocessing import LabelEncoder\n",
    "    from collections import defaultdict\n",
    "    import pickle\n",
    "    \n",
    "    session = snp.Session.builder.configs(state_dict['connection_parameters']).create()\n",
    "    session.use_warehouse(state_dict['compute_parameters']['default_warehouse'])\n",
    "\n",
    "    feature_df = session.table(state_dict['feature_table_name']).to_pandas()\n",
    "    #forecast_df = session.table(state_dict['forecast_table_name']).to_pandas()\n",
    "\n",
    "    session.close()\n",
    "\n",
    "    feature_df['DATE'] = pd.to_datetime(feature_df['DATE'])\n",
    "    feature_df.set_index('DATE', inplace=True)\n",
    "    \n",
    "    #forecast_df['DATE'] = pd.to_datetime(forecast_df['DATE'])\n",
    "    #forecast_df.set_index('DATE', inplace=True)\n",
    "\n",
    "    cat_cols = state_dict['cat_cols']\n",
    "    num_cols = [set(feature_df.columns)-set(cat_cols)]\n",
    "    state_dict['num_cols'] = num_cols\n",
    "\n",
    "    try:\n",
    "        with open(state_dict['le_file_name'], 'rb') as fh: \n",
    "            d=pickle.load(fh)\n",
    "        feature_df[cat_cols]=feature_df[cat_cols].apply(lambda x: d[x.name].transform(x))\n",
    "\n",
    "    except: \n",
    "        d = defaultdict(LabelEncoder)\n",
    "        feature_df[cat_cols]=feature_df[cat_cols].apply(lambda x: d[x.name].fit_transform(x))\n",
    "\n",
    "        with open(state_dict['le_file_name'], 'wb') as fh: \n",
    "            pickle.dump(d, fh)\n",
    "\n",
    "    return state_dict, feature_df\n",
    "\n",
    "def train_and_save(state_dict, feature_df):\n",
    "    import pandas as pd\n",
    "    from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "    \n",
    "    feature_df.sort_values(by='DATE', ascending=True, inplace=True)\n",
    "\n",
    "    train_df = feature_df.groupby('STATION_ID').head(-365)\n",
    "    valid_df = feature_df.groupby('STATION_ID').tail(365)\n",
    "\n",
    "    state_dict['cat_idxs'] = [feature_df.drop(columns=['COUNT'], axis=1).columns.get_loc(col) for col in state_dict['cat_cols']]\n",
    "    state_dict['cat_dims'] = list(feature_df.drop(columns=['COUNT'], axis=1).iloc[:, state_dict['cat_idxs']].nunique().values)\n",
    "\n",
    "    y_train = train_df['COUNT'].values.reshape(-1,1)\n",
    "    X_train = train_df.drop(columns ='COUNT', axis=1).values\n",
    "\n",
    "    y_valid = valid_df['COUNT'].values.reshape(-1,1)\n",
    "    X_valid = valid_df.drop(columns ='COUNT', axis=1).values\n",
    "    \n",
    "    model = TabNetRegressor(cat_idxs=state_dict['cat_idxs'], cat_dims=state_dict['cat_dims'])\n",
    "\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_valid, y_valid)],\n",
    "        max_epochs=1,\n",
    "        patience=100,\n",
    "        batch_size=2048, \n",
    "        virtual_batch_size=256,\n",
    "        num_workers=0,\n",
    "        drop_last=True)\n",
    "\n",
    "    model.save_model(state_dict['model_file_name'].split('.')[0])\n",
    "    \n",
    "    return state_dict\n",
    "\n",
    "def pred(state_dict, feature_df):\n",
    "    from pytorch_tabnet.tab_model import TabNetRegressor\n",
    "    import pandas as pd\n",
    "    from torch import tensor\n",
    "    \n",
    "    model = TabNetRegressor(cat_idxs=state_dict['cat_idxs'], cat_dims=state_dict['cat_dims'])\n",
    "\n",
    "    model.load_model(state_dict['model_file_name'])\n",
    "    \n",
    "    pred_df = feature_df.copy(deep=True)\n",
    "    \n",
    "    pred_df['PRED'] = model.predict(tensor(feature_df.drop(columns=['COUNT']).values)).round().astype('int')\n",
    "    \n",
    "    return state_dict, pred_df\n",
    "\n",
    "def forecast(state_dict, feature_df, forecast_df):\n",
    "\n",
    "    if len(state_dict['lag_values']) > 0:\n",
    "        for step in range(state_dict['forecast_steps']):\n",
    "            #station_id = df.iloc[-1]['STATION_ID']\n",
    "            future_date = df.iloc[-1]['DATE']+timedelta(days=1)\n",
    "            lags=[df.shift(lag-1).iloc[-1]['COUNT'] for lag in state_dict['lag_values']]\n",
    "            forecast=forecast_df.loc[forecast_df['DATE']==future_date.strftime('%Y-%m-%d')]\n",
    "            forecast=forecast.drop(labels='DATE', axis=1).values.tolist()[0]\n",
    "            features=[*lags, *forecast]\n",
    "            pred=round(model.predict(np.array([features]))[0][0])\n",
    "            row=[future_date, pred, *features, pred]\n",
    "            df.loc[len(df)]=row\n",
    "\n",
    "    return state_dict, pred_df\n",
    "\n",
    "def decode_and_write(state_dict, pred_df):\n",
    "    from snowflake import snowpark as snp\n",
    "    import pandas as pd\n",
    "    import pickle\n",
    "    \n",
    "    with open(state_dict['le_file_name'], 'rb') as fh: \n",
    "        d=pickle.load(fh)\n",
    "\n",
    "    pred_df[state_dict['cat_cols']] = pred_df[state_dict['cat_cols']].apply(lambda x: d[x.name].inverse_transform(x))\n",
    "\n",
    "\n",
    "    session = snp.Session.builder.configs(state_dict['connection_parameters']).create()\n",
    "    session.use_warehouse(state_dict['compute_parameters']['default_warehouse'])\n",
    "\n",
    "    session.create_dataframe(pred_df).write.mode('overwrite').save_as_table(state_dict['pred_table_name'])\n",
    "    \n",
    "    session.close()\n",
    "    \n",
    "    return state_dict\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    # Defining and parsing the command-line arguments\n",
    "    parser = argparse.ArgumentParser(description='airkube training')\n",
    "    parser.add_argument('--password', type=str)\n",
    "    parser.add_argument('--account', type=str)\n",
    "    parser.add_argument('--username', type=str)\n",
    "    parser.add_argument('--role', type=str)\n",
    "    parser.add_argument('--database', type=str)\n",
    "    parser.add_argument('--schema', type=str)\n",
    "    parser.add_argument('--feature_table_name', type=str)\n",
    "    parser.add_argument('--pred_table_name', type=str)\n",
    "    \n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Creating the directory where the output file will be created (the directory may or may not exist).\n",
    "    #Path(args.accuracy).parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    state_dict = {\"connection_parameters\": {\"password\": args.password},\n",
    "                  \"compute_parameters\" : {\"default_warehouse\": \"XSMALL_WH\"}}\n",
    "    state_dict['connection_parameters']['user'] = args.username\n",
    "    state_dict['connection_parameters']['account'] = args.account\n",
    "    state_dict['connection_parameters']['role'] = args.role\n",
    "    state_dict['connection_parameters']['database'] = args.database\n",
    "    state_dict['connection_parameters']['schema'] = args.schema\n",
    "    state_dict['feature_table_name'] = args.feature_table_name\n",
    "    state_dict['pred_table_name'] = args.pred_table_name\n",
    "    state_dict['model_file_name']='forecast_model.zip'\n",
    "    state_dict['le_file_name']='label_encoders.pkl'\n",
    "    state_dict[\"cat_cols\"] = ['STATION_ID', 'HOLIDAY']\n",
    "\n",
    "    load_state_dict, feature_df = load_and_encode(state_dict)\n",
    "    train_state_dict = train_and_save(load_state_dict, feature_df)\n",
    "    pred_state_dict, pred_df = pred(state_dict, feature_df)\n",
    "    state_dict = decode_and_write(state_dict, pred_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf33f62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_train import load_and_encode, train_and_save, pred, decode_and_write\n",
    "import json\n",
    "\n",
    "with open('./include/state.json') as sdf:\n",
    "    state_dict = json.load(sdf)    \n",
    "\n",
    "load_state_dict, feature_df = load_and_encode(state_dict)\n",
    "train_state_dict = train_and_save(load_state_dict, feature_df)\n",
    "pred_state_dict, pred_df = pred(state_dict, feature_df)\n",
    "state_dict = decode_and_write(state_dict, pred_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00450a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check output\n",
    "from snowflake import snowpark as snp\n",
    "\n",
    "session = snp.Session.builder.configs(state_dict['connection_parameters']).create()\n",
    "session.use_warehouse(state_dict['compute_parameters']['default_warehouse'])\n",
    "session.table(state_dict['pred_table_name']).show()\n",
    "session.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "089c96f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%writefile ./include/train.yaml\n",
    "# apiVersion: \"kubeflow.org/v1\"\n",
    "# kind: PyTorchJob\n",
    "# metadata:\n",
    "#   name: citibike-train\n",
    "#   namespace: citibike\n",
    "# spec:\n",
    "#   pytorchReplicaSpecs:\n",
    "#     Master:\n",
    "#       replicas: 1\n",
    "#       restartPolicy: Never\n",
    "#       template:\n",
    "#         spec:\n",
    "#           containers:\n",
    "#             - name: pytorch\n",
    "#               image: docker.io/mpgregor/airkube:latest\n",
    "#               imagePullPolicy: Always\n",
    "#               command:\n",
    "#                 - \"python\"\n",
    "#                 - \"/pipeline/load_train.py\"\n",
    "#                 - \"--account=\"\n",
    "#                 - \"--password=\"\n",
    "#                 - \"--username=\"\n",
    "#                 - \"--role=\"\n",
    "#                 - \"--database=\"\n",
    "#                 - \"--schema=\"\n",
    "#                 - \"--feature_table_name=\"\n",
    "#                 - \"--pred_table_name=\"\n",
    "                \n",
    "# !kubectl create namespace citibike\n",
    "# !kubectl create -f ./include/train.yaml \n",
    "# !kubectl delete pytorchjob citibike-train -n citibike\n",
    "# !kubectl delete namespace citibike                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ced08c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c126963c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd232d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from kubernetes.client import V1PodTemplateSpec\n",
    "from kubernetes.client import V1ObjectMeta\n",
    "from kubernetes.client import V1PodSpec\n",
    "from kubernetes.client import V1Container\n",
    "from kubernetes.client import V1ResourceRequirements\n",
    "\n",
    "from kubeflow.pytorchjob import constants\n",
    "from kubeflow.pytorchjob import utils\n",
    "from kubeflow.pytorchjob import V1ReplicaSpec\n",
    "from kubeflow.pytorchjob import V1PyTorchJob\n",
    "from kubeflow.pytorchjob import V1PyTorchJobSpec\n",
    "from kubeflow.pytorchjob import PyTorchJobClient\n",
    "\n",
    "#namespace = state_dict['k8s_namespace'] #utils.get_default_target_namespace()\n",
    "\n",
    "import json\n",
    "with open('./include/state.json') as sdf:\n",
    "    state_dict = json.load(sdf)\n",
    "\n",
    "from kubernetes import client, config\n",
    "\n",
    "config.load_kube_config('./include/kube_config.yaml')\n",
    "\n",
    "k8s_client = client.CoreV1Api()\n",
    "if state_dict['k8s_namespace'] not in [item.metadata.name for item in k8s_client.list_namespace().items]:\n",
    "    k8s_client.create_namespace(client.V1Namespace(metadata=client.V1ObjectMeta(name=state_dict['k8s_namespace'])))\n",
    "    \n",
    "\n",
    "container = V1Container(\n",
    "    name=\"pytorch\",\n",
    "    image=state_dict['train_image'],\n",
    "    image_pull_policy=\"Always\",\n",
    "    command=[\"python\", \n",
    "             \"/pipeline/load_train.py\",\n",
    "             \"--account=\"+state_dict['connection_parameters']['account'], \n",
    "             \"--password=\"+state_dict['connection_parameters']['password'],\n",
    "             \"--username=\"+state_dict['connection_parameters']['user'],\n",
    "             \"--role=\"+state_dict['connection_parameters']['role'], \n",
    "             \"--database=\"+state_dict['connection_parameters']['database'], \n",
    "             \"--schema=\"+state_dict['connection_parameters']['schema'], \n",
    "             \"--feature_table_name=\"+state_dict['feature_table_name'], \n",
    "             \"--pred_table_name=\"+state_dict['pred_table_name']\n",
    "            ]\n",
    ")\n",
    "\n",
    "master = V1ReplicaSpec(\n",
    "    replicas=1,\n",
    "    restart_policy=\"OnFailure\",\n",
    "    template=V1PodTemplateSpec(\n",
    "        spec=V1PodSpec(\n",
    "            containers=[container]\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "worker = V1ReplicaSpec(\n",
    "    replicas=1,\n",
    "    restart_policy=\"OnFailure\",\n",
    "    template=V1PodTemplateSpec(\n",
    "        spec=V1PodSpec(\n",
    "            containers=[container]\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "pytorchjob = V1PyTorchJob(\n",
    "    api_version=\"kubeflow.org/v1\",\n",
    "    kind=\"PyTorchJob\",\n",
    "    metadata=V1ObjectMeta(name=state_dict['train_job_name'], namespace=state_dict['k8s_namespace']),\n",
    "    spec=V1PyTorchJobSpec(\n",
    "        clean_pod_policy=\"None\",\n",
    "        pytorch_replica_specs={\"Master\": master} \n",
    "    )\n",
    ")\n",
    "\n",
    "pytorch_client = PyTorchJobClient()\n",
    "resp = pytorch_client.create(pytorchjob)\n",
    "\n",
    "#pytorch_client.get_job_status(name=resp['metadata']['name'], \n",
    "#                              namespace=resp['metadata']['namespace'])\n",
    "\n",
    "_ = pytorch_client.wait_for_condition(name=resp['metadata']['name'], \n",
    "                                       namespace=resp['metadata']['namespace'],\n",
    "                                       expected_condition='Succeeded')\n",
    "\n",
    "pytorch_client.delete(name=resp['metadata']['name'], \n",
    "                      namespace=resp['metadata']['namespace'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646f1e84",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897b61fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b74564",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile ./dags/k8s_test.py\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from airflow import DAG\n",
    "from airflow.configuration import conf\n",
    "from airflow.providers.cncf.kubernetes.operators.kubernetes_pod import KubernetesPodOperator\n",
    "\n",
    "default_args = {\n",
    "    'owner': 'airflow',\n",
    "    'depends_on_past': False,\n",
    "    'start_date': datetime(2019, 1, 1),\n",
    "    'email_on_failure': False,\n",
    "    'email_on_retry': False,\n",
    "    'retries': 1,\n",
    "    'retry_delay': timedelta(minutes=5),\n",
    "}\n",
    "\n",
    "namespace = conf.get('kubernetes', 'NAMESPACE')\n",
    "\n",
    "# This will detect the default namespace locally and read the\n",
    "# environment namespace when deployed to Astronomer.\n",
    "if namespace =='default':\n",
    "    config_file = '/usr/local/airflow/include/.kube/config'\n",
    "    in_cluster = False\n",
    "else:\n",
    "    in_cluster = True\n",
    "    config_file = None\n",
    "\n",
    "dag = DAG('example_kubernetes_pod', schedule_interval='@once', default_args=default_args)\n",
    "\n",
    "\n",
    "with dag:\n",
    "    KubernetesPodOperator(\n",
    "        namespace=namespace,\n",
    "        image=\"hello-world\",\n",
    "        labels={\"foo\": \"bar\"},\n",
    "        name=\"airflow-test-pod\",\n",
    "        task_id=\"task-one\",\n",
    "        in_cluster=in_cluster,  # if set to true, will look in the cluster, if false, looks for file\n",
    "        cluster_context=\"docker-desktop\",  # is ignored when in_cluster is set to True\n",
    "        config_file=config_file,\n",
    "        is_delete_operator_pod=True,\n",
    "        get_logs=True,\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a752458d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
